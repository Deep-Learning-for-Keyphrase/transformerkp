import logging
import pathlib
from collections import defaultdict
from typing import Union, Any, Dict, List

from datasets import get_dataset_split_names
from datasets import Dataset
from datasets import DatasetDict
from datasets import IterableDataset
from datasets import IterableDatasetDict
from datasets import load_dataset
from transformers import AutoTokenizer

from base import KPDataset
from args import KEDataArguments
from preprocessing import tokenize_and_align_labels

logger = logging.getLogger(__name__)


class KEDataset(KPDataset):

    _label_to_id: Dict = {"B": 0, "I": 1, "O": 2}
    _id_to_label: Dict = {0: "B", 1: "I", 2: "O"}
    _num_labels: int = 3

    def __init__(self, data_args: KEDataArguments):
        
        super().__init__()
        self._data_args: KEDataArguments = data_args
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = None
        self._validation: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = None
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = None
        # TODO: needs to be removed. No need of handling it over here
        # self._tokenizer: Any = AutoTokenizer.from_pretrained(data_args.tokenizer, use_fast=True, add_prefix_space=True)
        self._splits_to_load: Union[List[str], None] = self._data_args.splits
        self._text_column_name: Union[str, None] = (
            self._data_args.text_column_name if self._data_args is not None else None
        )
        self._label_column_name: Union[str, None] = (
            self._data_args.label_column_name if self._data_args is not None else None
        )
        # TODO: will remove after proper verificiation
        # if self._data_args.max_seq_length is None:
        #     self._data_args.max_seq_length = self._tokenizer.model_max_length
        # if self._data_args.max_seq_length > self._tokenizer.model_max_length:
        #     logger.warning(
        #         f"The max_seq_length passed ({self._data_args.max_seq_length}) is larger than the maximum length for the"
        #         f"model ({self._tokenizer.model_max_length}). Using max_seq_length={self._tokenizer.model_max_length}."
        #     )
        # self._max_seq_length: int = min(
        #     self._data_args.max_seq_length, self._tokenizer.model_max_length
        # )
        self._padding: Union[str, bool] = (
            "max_length" if self._data_args.pad_to_max_length else False
        )
        self.__preprocess_function = self._data_args.preprocess_func
        self._datasets: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = None
        self.__load_kp_datasets()

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

    @property
    def text_column_name(self) -> Union[str, None]:
        """Get the column name of the input text from which keyphrases needs to be extracted"""
        return self._text_column_name

    @property
    def label_column_name(self) -> Union[str, None]:
        """Get the column name of the column which contains the BIO labels of the tokens"""
        return self._label_column_name

    @property
    def label_to_id(self) -> Dict:
        """Get the label to id mapping used for creating the dataset"""
        return self._label_to_id

    @property
    def padding(self) -> Union[str, bool]:
        """Get the padding strategy to be used for preprocessing the dataset for training and evaluation"""
        return self._padding

    @property
    def label_all_tokens(self) -> bool:
        """Get the label_all_tokens property which decides whether to put the label for one word on all
        sub-words generated by that word or just on the one (in which case the other tokens will have a padding index).
        This will be later used while preprocessing the dataset for training and evaluation
        """
        return self._data_args.label_all_tokens

    @property
    def max_seq_length(self) -> int:
        """Gets the max sequence length to be used while preprocessing the dataset for training and evaluation"""
        return self._max_seq_length

    def __load_kp_datasets(self) -> None:
        """Loads the training, validation and test splits from either an existing dataset from Huggingface hub or from provided files.

        """
        if self._data_args.dataset_name is not None:
            dataset_splits = get_dataset_split_names(self._data_args.dataset_name, "extraction")
            self._splits_to_load = list(set(dataset_splits).intersection(set(self._data_args.splits)))
            logger.info(f"Only loading the following splits {self._splits_to_load}")
            # Downloading and loading a dataset from the hub.
            self._datasets = load_dataset(
                self._data_args.dataset_name,
                self._data_args.dataset_config_name,
                split=self._splits_to_load,
                cache_dir=self._data_args.cache_dir,
            )
        else:
            # TODO: What if the train, validation and test files are in different formats - we cannot allow it.
            data_files = {}
            if self._data_args.train_file is not None:
                data_files["train"] = self._data_args.train_file
                extension = pathlib.Path(self._data_args.train_file).suffix.replace(".", "")
                logger.info(f"Loaded training data from {self._data_args.train_file}")
            if self._data_args.validation_file is not None:
                data_files["validation"] = self._data_args.validation_file
                extension = pathlib.Path(self._data_args.validation_file).suffix.replace(".", "")
                logger.info(f"Loaded validation data from {self._data_args.validation_file}")
            if self._data_args.test_file is not None:
                data_files["test"] = self._data_args.test_file
                extension = pathlib.Path(self._data_args.test_file).suffix.replace(".", "")
                logger.info(f"Loaded test data from {self._data_args.test_file}")

            logger.info(f"Only loading the following splits {self._splits_to_load}")
            self._datasets = load_dataset(
                extension, 
                data_files=data_files, 
                cache_dir=self._data_args.cache_dir,
                split=self._splits_to_load,
            )

        if self.__preprocess_function:
            if self._datasets:
                self._datasets = self._datasets.map(
                    self.__preprocess_function,
                    num_proc=self._data_args.preprocessing_num_workers,
                )
                logger.info(f"preprocessing done with the provided customized preprocessing function")
        
        self._datasets = self.__create_dataset_dict_from_splits(self._datasets)

        if "train" in self._datasets:
            column_names = self._datasets["train"].column_names
        elif "validation" in self._datasets:
            column_names = self._datasets["validation"].column_names
        elif "test" in self._datasets:
            column_names = self._datasets["test"].column_names
        else:
            raise AssertionError(
                "neither train, validation or test dataset is available"
            )

        if self._text_column_name is None:
            self._text_column_name = (
                # TODO: convey this information properly in the documentation
                "document" if "document" in column_names else column_names[1]
            )  # either document or 2nd column as text i/p

        assert self._text_column_name in column_names

        if self._label_column_name is None:
            self._label_column_name = (
                "doc_bio_tags" if "doc_bio_tags" in column_names else None
            )
            # TODO: convey this information properly in the documentation
            if len(column_names) > 2:
                self._label_column_name = column_names[2]

        if self._label_column_name is not None:
            assert self._label_column_name in column_names

        # TODO: to be done outside before training
        # self._datasets = tokenize_and_align_labels(
        #     datasets=self._datasets,
        #     text_column_name=self._text_column_name,
        #     label_column_name=self._label_column_name,
        #     tokenizer=self._tokenizer,
        #     label_to_id=self._label_to_id,
        #     padding=self._padding,
        #     label_all_tokens=self._data_args.label_all_tokens,
        #     max_seq_len=self._max_seq_length,
        #     num_workers=self._data_args.preprocessing_num_workers,
        #     overwrite_cache=self._data_args.overwrite_cache,
        # )

        if "train" in self._datasets:
            self._train = self._datasets["train"]
        if "validation" in self._datasets:
            self._validation = self._datasets["validation"]
        if "test" in self._datasets:
            self._test = self._datasets["test"]

    def __create_dataset_dict_from_splits(self, data_splits: List[Dataset]) -> DatasetDict:
        data_dict = defaultdict(Dataset)
        for split_name, data_split in zip(self._splits_to_load, data_splits):
            data_dict[split_name] = data_split
        return DatasetDict(data_dict)


if __name__ == "__main__":

    # TODO: to be removed. Temporary code for testing the module
    # testing the general KEDataArguments class
    from args import KEDataArguments
    
    data_args = KEDataArguments(
        dataset_name="midas/inspec",
        splits=["train", "test"],
        cache_dir="/data/hf_datasets"
    )

    ke_data = KEDataset(data_args)
    print(ke_data._datasets)
    print(ke_data.train)
    print(ke_data.validation)
    print(ke_data.test)

    assert ke_data.test.num_rows == 500
    assert ke_data.train.num_rows == 1000
    assert ke_data.validation is None

    # testing out loading of a dataset from user provided files
    # path to small files for testing the proper loading of custom files to be loaded into a dataset
    train_file = "../../../tests/resources/data/train.json"
    validation_file = "../../../tests/resources/data/valid.json"
    test_file = "../../../tests/resources/data/test.json"

    custom_data_args = KEDataArguments(
        splits=["train", "test"],
        cache_dir="/data/hf_datasets",
        train_file=train_file,
        validation_file=validation_file,
        test_file=test_file,
    )

    custom_ke_data = KEDataset(custom_data_args)
    print(custom_ke_data._datasets)
    print(custom_ke_data.train)
    print(custom_ke_data.validation)
    print(custom_ke_data.test)

    assert custom_ke_data.test.num_rows == 5
    assert custom_ke_data.train.num_rows == 20
    assert custom_ke_data.validation is None

    # test custom loading of NUS dataset
    from args import NUSKEDataArguments

    nus_data_args = NUSKEDataArguments()
    nus_ke_data = KEDataset(nus_data_args)
    print(nus_ke_data._datasets)
    print(nus_ke_data.train)
    print(nus_ke_data.validation)
    print(nus_ke_data.test)

    assert nus_ke_data.test.num_rows == 211
    assert nus_ke_data.train is None
    assert nus_ke_data.validation is None

    # test custom loading of Inspec dataset
    from args import InspecKEDataArguments

    inspec_data_args = InspecKEDataArguments()
    inspec_ke_data = KEDataset(inspec_data_args)
    print(inspec_ke_data._datasets)
    print(inspec_ke_data.train)
    print(inspec_ke_data.validation)
    print(inspec_ke_data.test)

    assert inspec_ke_data.test.num_rows == 500
    assert inspec_ke_data.train.num_rows == 1000
    assert inspec_ke_data.validation.num_rows == 500
