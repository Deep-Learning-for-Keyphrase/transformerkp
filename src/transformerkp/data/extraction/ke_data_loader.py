import logging
import pathlib
from collections import defaultdict
from typing import Union, Dict, List

from datasets import get_dataset_split_names
from datasets import Dataset
from datasets import DatasetDict
from datasets import IterableDataset
from datasets import IterableDatasetDict
from datasets import load_dataset

from transformerkp.data.base import KPDataset
from transformerkp.data.extraction.ke_data_args import KEDataArguments
from transformerkp.data.extraction import ke_data_args

logger = logging.getLogger(__name__)


class KEDataset(KPDataset):

    _label_to_id: Dict = {"B": 0, "I": 1, "O": 2}
    _id_to_label: Dict = {0: "B", 1: "I", 2: "O"}
    _num_labels: int = 3

    def __init__(self, data_args: KEDataArguments):
        
        super().__init__()
        self.data_args: KEDataArguments = data_args
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = None
        self._validation: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = None
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = None
        # TODO: needs to be removed. No need of handling it over here
        # self._tokenizer: Any = AutoTokenizer.from_pretrained(data_args.tokenizer, use_fast=True, add_prefix_space=True)
        self._splits_to_load: Union[List[str], None] = self.data_args.splits
        self._text_column_name: Union[str, None] = (
            self.data_args.text_column_name if self.data_args is not None else None
        )
        self._label_column_name: Union[str, None] = (
            self.data_args.label_column_name if self.data_args is not None else None
        )
        # TODO: will remove after proper verificiation
        # if self._data_args.max_seq_length is None:
        #     self._data_args.max_seq_length = self._tokenizer.model_max_length
        # if self._data_args.max_seq_length > self._tokenizer.model_max_length:
        #     logger.warning(
        #         f"The max_seq_length passed ({self._data_args.max_seq_length}) is larger than the maximum length for the"
        #         f"model ({self._tokenizer.model_max_length}). Using max_seq_length={self._tokenizer.model_max_length}."
        #     )
        # self._max_seq_length: int = min(
        #     self._data_args.max_seq_length, self._tokenizer.model_max_length
        # )
        self._padding: Union[str, bool] = (
            "max_length" if self.data_args.pad_to_max_length else False
        )
        self.__preprocess_function = self.data_args.preprocess_func
        self._datasets: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = None
        self.__load_kp_datasets()

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

    @property
    def text_column_name(self) -> Union[str, None]:
        """Get the column name of the input text from which keyphrases needs to be extracted"""
        return self._text_column_name

    @property
    def label_column_name(self) -> Union[str, None]:
        """Get the column name of the column which contains the BIO labels of the tokens"""
        return self._label_column_name

    @property
    def label_to_id(self) -> Dict:
        """Get the label to id mapping used for creating the dataset"""
        return self._label_to_id

    @property
    def padding(self) -> Union[str, bool]:
        """Get the padding strategy to be used for preprocessing the dataset for training and evaluation"""
        return self._padding

    @property
    def label_all_tokens(self) -> bool:
        """Get the label_all_tokens property which decides whether to put the label for one word on all
        sub-words generated by that word or just on the one (in which case the other tokens will have a padding index).
        This will be later used while preprocessing the dataset for training and evaluation
        """
        return self._data_args.label_all_tokens

    @property
    def max_seq_length(self) -> int:
        """Gets the max sequence length to be used while preprocessing the dataset for training and evaluation"""
        return self._max_seq_length

    def __load_kp_datasets(self) -> None:
        """Loads the training, validation and test splits from either an existing dataset from Huggingface hub or from provided files.

        """
        if self.data_args.dataset_name is not None:
            dataset_splits = get_dataset_split_names(self.data_args.dataset_name, "extraction")
            self._splits_to_load = list(set(dataset_splits).intersection(set(self.data_args.splits)))
            logger.info(f"Only loading the following splits {self._splits_to_load}")
            # Downloading and loading a dataset from the hub.
            self._datasets = load_dataset(
                self.data_args.dataset_name,
                self.data_args.dataset_config_name,
                split=self._splits_to_load,
                cache_dir=self.data_args.cache_dir,
            )
        else:
            # TODO: What if the train, validation and test files are in different formats - we cannot allow it.
            data_files = {}
            if self.data_args.train_file is not None:
                data_files["train"] = self.data_args.train_file
                extension = pathlib.Path(self.data_args.train_file).suffix.replace(".", "")
                logger.info(f"Loaded training data from {self.data_args.train_file}")
            if self.data_args.validation_file is not None:
                data_files["validation"] = self.data_args.validation_file
                extension = pathlib.Path(self.data_args.validation_file).suffix.replace(".", "")
                logger.info(f"Loaded validation data from {self.data_args.validation_file}")
            if self.data_args.test_file is not None:
                data_files["test"] = self.data_args.test_file
                extension = pathlib.Path(self.data_args.test_file).suffix.replace(".", "")
                logger.info(f"Loaded test data from {self.data_args.test_file}")

            logger.info(f"Only loading the following splits {self._splits_to_load}")
            self._datasets = load_dataset(
                extension, 
                data_files=data_files, 
                cache_dir=self.data_args.cache_dir,
                split=self._splits_to_load,
            )

        if self.__preprocess_function:
            if self._datasets:
                self._datasets = self._datasets.map(
                    self.__preprocess_function,
                    num_proc=self.data_args.preprocessing_num_workers,
                )
                logger.info(f"preprocessing done with the provided customized preprocessing function")
        
        self._datasets = self.__create_dataset_dict_from_splits(self._datasets)

        if "train" in self._datasets:
            column_names = self._datasets["train"].column_names
        elif "validation" in self._datasets:
            column_names = self._datasets["validation"].column_names
        elif "test" in self._datasets:
            column_names = self._datasets["test"].column_names
        else:
            raise AssertionError(
                "neither train, validation or test dataset is available"
            )

        if self._text_column_name is None:
            self._text_column_name = (
                # TODO: convey this information properly in the documentation
                "document" if "document" in column_names else column_names[1]
            )  # either document or 2nd column as text i/p

        assert self._text_column_name in column_names

        if self._label_column_name is None:
            self._label_column_name = (
                "doc_bio_tags" if "doc_bio_tags" in column_names else None
            )
            # TODO: convey this information properly in the documentation
            if len(column_names) > 2:
                self._label_column_name = column_names[2]

        if self._label_column_name is not None:
            assert self._label_column_name in column_names

        # TODO: to be done outside before training
        # self._datasets = tokenize_and_align_labels(
        #     datasets=self._datasets,
        #     text_column_name=self._text_column_name,
        #     label_column_name=self._label_column_name,
        #     tokenizer=self._tokenizer,
        #     label_to_id=self._label_to_id,
        #     padding=self._padding,
        #     label_all_tokens=self._data_args.label_all_tokens,
        #     max_seq_len=self._max_seq_length,
        #     num_workers=self._data_args.preprocessing_num_workers,
        #     overwrite_cache=self._data_args.overwrite_cache,
        # )

        if "train" in self._datasets:
            self._train = self._datasets["train"]
        if "validation" in self._datasets:
            self._validation = self._datasets["validation"]
        if "test" in self._datasets:
            self._test = self._datasets["test"]

    def __create_dataset_dict_from_splits(self, data_splits: List[Dataset]) -> DatasetDict:
        data_dict = defaultdict(Dataset)
        for split_name, data_split in zip(self._splits_to_load, data_splits):
            data_dict[split_name] = data_split
        return DatasetDict(data_dict)


class InspecKEDataset(KPDataset):
    """Class for loading the Inspec dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["train", "validation", "test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir: Union[str, None] = None,
    ):
        """Init method for InspecKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.InspecKEDataArguments = ke_data_args.InspecKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class NUSKEDataset(KPDataset):
    """Class for loading the NUS dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for NUSKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.NUSKEDataArguments = ke_data_args.NUSKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class KDDKEDataset(KPDataset):
    """Class for loading the KDD dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for KDDKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.KDDKEDataArguments = ke_data_args.KDDKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class KrapivinKEDataset(KPDataset):
    """Class for loading the Krapivin dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for KrapivinKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.KrapivinKEDataArguments = ke_data_args.KrapivinKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test


class KP20KKEDataset(KPDataset):
    """Class for loading the KP20K dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["train", "validation", "test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for KP20KKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.KP20KKEDataArguments = ke_data_args.KP20KKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class WWWKEDataset(KPDataset):
    """Class for loading the WWW dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for WWWKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.WWWKEDataArguments = ke_data_args.WWWKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class LDKP3KSmallKEDataset(KEDataset):
    """Class for loading the LDKP3K small dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 4096,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        super().__init__()
        self._data_args: ke_data_args.LDKP3KSmallKEDataArguments = ke_data_args.LDKP3KSmallKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class LDKP3KMediumKEDataset(KEDataset):
    """Class for loading the LDKP3K medium dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 4096,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        super().__init__()
        self._data_args: ke_data_args.LDKP3KMediumKEDataArguments = ke_data_args.LDKP3KMediumKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test


    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class LDKP3KLargeKEDataset(KEDataset):
    """Class for loading the LDKP3K large dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 4096,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        super().__init__()
        self._data_args: ke_data_args.LDKP3KLargeKEDataArguments = ke_data_args.LDKP3KLargeKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test


    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class LDKP10KSmallKEDataset(KEDataset):
    """Class for loading the LDKP10K small dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 4096,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        super().__init__()
        self._data_args: ke_data_args.LDKP10KSmallKEDataArguments = ke_data_args.LDKP10KSmallKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test


    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class LDKP10KMediumKEDataset(KEDataset):
    """Class for loading the LDKP10K medium dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 4096,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        super().__init__()
        self._data_args: ke_data_args.LDKP10KMediumKEDataArguments = ke_data_args.LDKP10KMediumKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test


    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class LDKP10KLargeKEDataset(KEDataset):
    """Class for loading the LDKP10K large dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 4096,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        super().__init__()
        self._data_args: ke_data_args.LDKP10KLargeKEDataArguments = ke_data_args.LDKP10KLargeKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class KPTimesKEDataset(KPDataset):
    """Class for loading the KPTimes dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["train", "validation", "test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for KPTimesKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.KPTimesKEDataArguments = ke_data_args.KPTimesKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test


class OpenKPKEDataset(KPDataset):
    """Class for loading the OpenKP dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["train", "validation", "test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for OpenKPKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.OpenKPKEDataArguments = ke_data_args.OpenKPKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test


class SemEval2010KEDataset(KPDataset):
    """Class for loading the SemEval 2010 dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["train", "test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for SemEval2010KEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.SemEval2010KEDataArguments = ke_data_args.SemEval2010KEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test


class SemEval2017KEDataset(KPDataset):
    """Class for loading the SemEval 2017 dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["train", "validation", "test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for SemEval2017KEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.SemEval2017DataArguments = ke_data_args.SemEval2017KEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class KPCrowdKEDataset(KPDataset):
    """Class for loading the KPCrowd dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["train", "test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for KPCrowdKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.KPCrowdKEDataArguments = ke_data_args.KPCrowdKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test


class DUC2001KEDataset(KPDataset):
    """Class for loading the DUC 2001 dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for DUC2001KEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.KPCrowdKEDataArguments = ke_data_args.DUC2001KEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test


class CSTRKEDataset(KPDataset):
    """Class for loading the CSTR dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["train", "test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for CSTRKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.CSTRKEDataArguments = ke_data_args.CSTRKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class PubMedKEDataset(KPDataset):
    """Class for loading the Pub Med dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for PubMedKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.PubMedKEDataArguments = ke_data_args.PubMedKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test

class CiteulikeKEDataset(KPDataset):
    """Class for loading the Citeulike dataset from Huggingface Hub"""
    def __init__(
            self,
            splits: list = ["test"],
            max_seq_length: int = 512,
            label_all_tokens: bool = True,
            cache_dir=None,
    ):
        """Init method for CiteulikeKEDataset

        Args:
            splits (list): Names of the data splits to be loaded. For example, sometimes, one might only need
                to load the test split of the data.
            max_seq_length (int): The maximum total input sequence length after tokenization. Sequences longer than
                this will be truncated, sequences shorter will be padded.
            label_all_tokens (bool): Whether to put the label for one word on all sub-words generated by that word or
                just on the one, in which case the other tokens will have a padding index (default:True).
            cache_dir (str): Provide the name of a path for the cache dir. It is used to store the results
                of the computation (default: None).
        """
        super().__init__()
        self._data_args: ke_data_args.CiteulikeKEDataArguments = ke_data_args.CiteulikeKEDataArguments()
        self._data_args.splits = splits
        self._data_args.max_seq_length = max_seq_length
        self._data_args.label_all_tokens = label_all_tokens
        self._data_args.cache_dir = cache_dir
        self._dataset: KEDataset = KEDataset(self._data_args)
        self._train: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.train
        self._validation: Union[
            DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.validation
        self._test: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None] = self._dataset.test

    @property
    def train(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the train split"""
        return self._train

    @property
    def validation(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the validation split"""
        return self._validation

    @property
    def test(self) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset, None]:
        """Gets the test split"""
        return self._test
